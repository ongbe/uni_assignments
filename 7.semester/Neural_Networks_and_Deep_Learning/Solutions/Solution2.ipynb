{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2\n",
    "\n",
    "**Submission deadline: last lab session before or on Wednesday 25.10.17**\n",
    "\n",
    "**Points: 13 + 1 bonus points**\n",
    "\n",
    "\n",
    "## Downloading this notebook\n",
    "\n",
    "This assignment is an Jupyter notebook. Download it by cloning https://github.com/janchorowski/nn_assignments. Follow the instructions in its README for instructions. Whenever possible, add your solutions to the notebook.\n",
    "\n",
    "Please email us about any problems with it - we will try to correct them quickly. Also, please do not hesitate to use GitHub’s pull requests to send us corrections!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1 [2p] Bayes' Theorem\n",
    "\n",
    "Bayes' theorem allows to reason about conditional probabilities of causes and their effects:\n",
    "\n",
    "\\begin{equation}\n",
    "p(A,B)=p(A|B)p(B)=p(B|A)p(A)\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "p(A|B) = \\frac{p(B|A)p(A)}{p(B)}\n",
    "\\end{equation}\n",
    "\n",
    "Bayes' theorem allows us to reason about probabilities of causes, when\n",
    "we observe their results.  Instead of directly answering the hard\n",
    "question $p(\\text{cause}|\\text{result})$ we can instead separately\n",
    "work out the marginal probabilities of causes $p(\\text{cause})$ and\n",
    "carefully study their effects $p(\\text{effect}|\\text{cause})$.\n",
    "\n",
    "Solve the following using Bayes' theorem.\n",
    "\n",
    "1. **[1p]** There are two boxes on the table: box \\#1 holds two\n",
    "  black balls and eight red ones, box \\#2 holds 5 black ones and\n",
    "  5 red ones. We pick a box at random (with equal probabilities),\n",
    "  and then a ball from that box.\n",
    "  1. What is the probability, that the\n",
    "  ball came from box \\#1 if we happened to pick a red ball?\n",
    "  \n",
    "1. **[1p]** The government has started a preventive program of\n",
    "  mandatory tests for the Ebola virus. Mass testing method is\n",
    "  imprecise, yielding 1% of false positives (healthy, but the test\n",
    "  indicates the virus) and 1% of false negatives (\n",
    "  having the virus but healthy according to test results).\n",
    "  As Ebola is rather infrequent, lets assume that it occurs in\n",
    "  one in a million people in Europe.\n",
    "  1. What is the probability,\n",
    "  that a random European, who has been tested positive for Ebola\n",
    "  virus, is indeed a carrier?\n",
    "  2. Suppose we have an additional information, that the person has just\n",
    "  arrived from a country where one in a thousand people is a carrier.\n",
    "  How much will be the increase in probability?\n",
    "  3. How accurate should be the test, for a 80% probability of true\n",
    "  positive in a European?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\\# TODO Fill in the calculations instead of doing them by hand <br>\n",
    "\\# TODO Be precise and remember to add comments! <br>\n",
    "**1:** <br>\n",
    "$A$ - event signifies that a ball came from box \\#1 <br>\n",
    "$B$ - event signifies that a red ball is picked <br>\n",
    "\n",
    "\\begin{equation}\n",
    "p(A) = \\frac{1}{2}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "p(B) = \\frac{13}{20}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "p(B|A) = \\frac{8}{10} = \\frac{4}{5}\n",
    "\\end{equation}\n",
    "\n",
    "**a)** <br>\n",
    "\\begin{equation}\n",
    "p(A|B) = \\frac{p(B|A)p(A)}{p(B)} = \\frac{\\frac{4}{5}\\times\\frac{1}{2}}{\\frac{13}{20}} = \\frac{8}{13}\n",
    "\\end{equation}\n",
    "\n",
    "**2:** <br>\n",
    "$A$ - event signifies that a test is positive <br>\n",
    "$B$ - event signifies that a test is negative <br>\n",
    "$E$ - event signifies that Ebola occurs for a patient <br>\n",
    "$D$ - test accuracy <br>\n",
    "\n",
    "**a)** <br>\n",
    "\\begin{equation}\n",
    "p(A) = D \\times p(E)+(1-D)(1-p(E))\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "p(B) = 1-p(A)\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "p(E) = \\frac{1}{10^6}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "D = p(A|E) = 1-\\frac{1}{10^2} = \\frac{99}{100}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "p(E|A) = \\frac{p(A|E)p(E)}{p(A)} = \\frac{\\frac{99}{100}\\times\\frac{1}{10^6}}{(1-\\frac{1}{10^2})\\frac{1}{10^6}+\\frac{1}{10^2}(1-\\frac{1}{10^6})} = 9.899029895070284e-05 \\approx 0.009899\\%\n",
    "\\end{equation}\n",
    "\n",
    "**b)** <br>\n",
    "\\begin{equation}\n",
    "p(E)' = \\frac{1}{10^3}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "p(E|A)' = \\frac{p(A|E)p(E)'}{p(A)} = \\frac{\\frac{99}{100}\\times\\frac{1}{10^3}}{(1-\\frac{1}{10^2})\\frac{1}{10^3}+\\frac{1}{10^2}(1-\\frac{1}{10^3})} = 0.09016393442622951 \\approx 9.0164\\%\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{p(E|A)'}{p(E|A)} = \\frac{0.09016393442622951}{9.899029895070284e-05} = 910.8360655737704 \\approx 91000\\%\n",
    "\\end{equation}\n",
    "\n",
    "**c)** <br>\n",
    "\\begin{equation}\n",
    "p(E|A) = \\frac{p(A|E)p(E)}{p(A)} = \\frac{\\frac{D}{10^6}}{\\frac{D}{10^6}+(1-D)(1-\\frac{1}{10^6})} = 0.8 \n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "D = \\frac{10^6 \\times 0.8 \\times (1-\\frac{1}{10^6})}{1-0.8+10^6 \\times 0.8 \\times (1-\\frac{1}{10^6})} = 0.9999999999997501 \\approx 99.999999999975\\%\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2 [2p + 1b] Naive Bayes Classifier\n",
    "\n",
    "The Bayes' theorem allows us to construct a classifier in which we\n",
    "model how the data is generated. Here we will describe a\n",
    "simple and popular example of such a classifier called the naive\n",
    "Bayes classifier.  Despite its simplicity It is quite effective for\n",
    "classification of text documents (e.g. as spam and non-spam).\n",
    "\n",
    "Let a document be a sequence of words $D=W_1,W_2,\\ldots,W_n$ \n",
    "We will model generation of text documents as a two-stage process.\n",
    "First, document category $C_j$ is drawn at random with probability\n",
    "$p(C_j)$, also called the *a priori* probability.\n",
    "To define the class-conditional probability\n",
    "$p(D|C_j)$, we will make a simplifying (naive)\n",
    "assumption, that every word in the document is drawn independently at\n",
    "random with probability $p(W_i|C)$:\n",
    "\n",
    "\\begin{equation*}\n",
    "  p(D|C_j) = p(W_1,W_2,\\ldots,W_n | C_j) \\approx p(W_1|C_j)p(W_2|C_j)\\ldots p(W_n|C_j).\n",
    "\\end{equation*}\n",
    "\n",
    "To infer the class of a document we apply the Bayes theorem:\n",
    "\\begin{equation*}   p(C_j|D) = \\frac{p(D|C_j)p(C_j)}{p(D)} = \\frac{p(C_j)p(W_1|C_j)p(W_2|C_j)\\ldots p(W_n|C_j)}{p(D)}.\n",
    "\\end{equation*}\n",
    "Please note that since we assumed only a finite number of classes,\n",
    "we can compute the term $p(D)$ by making sure that the *a\n",
    "posteriori probabilities* $p(C_j|D)$ sum to $1$ over all classes.\n",
    "\n",
    "In this exercise we will try to mimic the language-guessing feature\n",
    "of [Google Translate](https://translate.google.com/), although\n",
    "on a much smaller scale.  We are given an input which is a\n",
    "lower-case sequence of characters (such as *\"some people like\n",
    "pineapple on their pizza\"*), and we determine whether the\n",
    "sequence's language is English, Polish or Spanish.\n",
    "We will treat each character as a separate observation.\n",
    "The numbers are taken from [Wikipedia article on letter frequency](https://en.wikipedia.org/wiki/Letter_frequency#Relative_frequencies_of_letters_in_other_languages). We display the first few rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English</th>\n",
       "      <th>French</th>\n",
       "      <th>German</th>\n",
       "      <th>Spanish</th>\n",
       "      <th>Portuguese</th>\n",
       "      <th>Esperanto</th>\n",
       "      <th>Italian</th>\n",
       "      <th>Turkish</th>\n",
       "      <th>Swedish</th>\n",
       "      <th>Polish</th>\n",
       "      <th>Dutch</th>\n",
       "      <th>Danish</th>\n",
       "      <th>Icelandic</th>\n",
       "      <th>Finnish</th>\n",
       "      <th>Czech</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>8.167</td>\n",
       "      <td>7.636</td>\n",
       "      <td>6.516</td>\n",
       "      <td>11.525</td>\n",
       "      <td>14.634</td>\n",
       "      <td>12.117</td>\n",
       "      <td>11.745</td>\n",
       "      <td>12.920</td>\n",
       "      <td>9.383</td>\n",
       "      <td>10.503</td>\n",
       "      <td>7.486</td>\n",
       "      <td>6.025</td>\n",
       "      <td>10.110</td>\n",
       "      <td>12.217</td>\n",
       "      <td>8.421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b</th>\n",
       "      <td>1.492</td>\n",
       "      <td>0.901</td>\n",
       "      <td>1.886</td>\n",
       "      <td>2.215</td>\n",
       "      <td>1.043</td>\n",
       "      <td>0.980</td>\n",
       "      <td>0.927</td>\n",
       "      <td>2.844</td>\n",
       "      <td>1.535</td>\n",
       "      <td>1.740</td>\n",
       "      <td>1.584</td>\n",
       "      <td>2.000</td>\n",
       "      <td>1.043</td>\n",
       "      <td>0.281</td>\n",
       "      <td>0.822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c</th>\n",
       "      <td>2.782</td>\n",
       "      <td>3.260</td>\n",
       "      <td>2.732</td>\n",
       "      <td>4.019</td>\n",
       "      <td>3.882</td>\n",
       "      <td>0.776</td>\n",
       "      <td>4.501</td>\n",
       "      <td>1.463</td>\n",
       "      <td>1.486</td>\n",
       "      <td>3.895</td>\n",
       "      <td>1.242</td>\n",
       "      <td>0.565</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.281</td>\n",
       "      <td>0.740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d</th>\n",
       "      <td>4.253</td>\n",
       "      <td>3.669</td>\n",
       "      <td>5.076</td>\n",
       "      <td>5.010</td>\n",
       "      <td>4.992</td>\n",
       "      <td>3.044</td>\n",
       "      <td>3.736</td>\n",
       "      <td>5.206</td>\n",
       "      <td>4.702</td>\n",
       "      <td>3.725</td>\n",
       "      <td>5.933</td>\n",
       "      <td>5.858</td>\n",
       "      <td>1.575</td>\n",
       "      <td>1.043</td>\n",
       "      <td>3.475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e</th>\n",
       "      <td>12.702</td>\n",
       "      <td>14.715</td>\n",
       "      <td>16.396</td>\n",
       "      <td>12.181</td>\n",
       "      <td>12.570</td>\n",
       "      <td>8.995</td>\n",
       "      <td>11.792</td>\n",
       "      <td>9.912</td>\n",
       "      <td>10.149</td>\n",
       "      <td>7.352</td>\n",
       "      <td>18.910</td>\n",
       "      <td>15.453</td>\n",
       "      <td>6.418</td>\n",
       "      <td>7.968</td>\n",
       "      <td>7.562</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   English  French  German  Spanish  Portuguese  Esperanto  Italian  Turkish  \\\n",
       "a    8.167   7.636   6.516   11.525      14.634     12.117   11.745   12.920   \n",
       "b    1.492   0.901   1.886    2.215       1.043      0.980    0.927    2.844   \n",
       "c    2.782   3.260   2.732    4.019       3.882      0.776    4.501    1.463   \n",
       "d    4.253   3.669   5.076    5.010       4.992      3.044    3.736    5.206   \n",
       "e   12.702  14.715  16.396   12.181      12.570      8.995   11.792    9.912   \n",
       "\n",
       "   Swedish  Polish   Dutch  Danish  Icelandic  Finnish  Czech  \n",
       "a    9.383  10.503   7.486   6.025     10.110   12.217  8.421  \n",
       "b    1.535   1.740   1.584   2.000      1.043    0.281  0.822  \n",
       "c    1.486   3.895   1.242   0.565      0.000    0.281  0.740  \n",
       "d    4.702   3.725   5.933   5.858      1.575    1.043  3.475  \n",
       "e   10.149   7.352  18.910  15.453      6.418    7.968  7.562  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from StringIO import StringIO\n",
    "\n",
    "wiki_table = u\"\"\"English|French|German|Spanish|Portuguese|Esperanto|Italian|Turkish|Swedish|Polish|Dutch|Danish|Icelandic|Finnish|Czech\\na|8.167|7.636|6.516|11.525|14.634|12.117|11.745|12.920|9.383|10.503|7.486|6.025|10.110|12.217|8.421\\nb|1.492|0.901|1.886|2.215|1.043|0.980|0.927|2.844|1.535|1.740|1.584|2.000|1.043|0.281|0.822\\nc|2.782|3.260|2.732|4.019|3.882|0.776|4.501|1.463|1.486|3.895|1.242|0.565|0|0.281|0.740\\nd|4.253|3.669|5.076|5.010|4.992|3.044|3.736|5.206|4.702|3.725|5.933|5.858|1.575|1.043|3.475\\ne|12.702|14.715|16.396|12.181|12.570|8.995|11.792|9.912|10.149|7.352|18.91|15.453|6.418|7.968|7.562\\nf|2.228|1.066|1.656|0.692|1.023|1.037|1.153|0.461|2.027|0.143|0.805|2.406|3.013|0.194|0.084\\ng|2.015|0.866|3.009|1.768|1.303|1.171|1.644|1.253|2.862|1.731|3.403|4.077|4.241|0.392|0.092\\nh|6.094|0.737|4.577|0.703|0.781|0.384|0.636|1.212|2.090|1.015|2.380|1.621|1.871|1.851|1.356\\ni|6.966|7.529|6.550|6.247|6.186|10.012|10.143|9.600|5.817|8.328|6.499|6.000|7.578|10.817|6.073\\nj|0.153|0.613|0.268|0.493|0.397|3.501|0.011|0.034|0.614|1.836|1.46|0.730|1.144|2.042|1.433\\nk|0.772|0.049|1.417|0.011|0.015|4.163|0.009|5.683|3.140|2.753|2.248|3.395|3.314|4.973|2.894\\nl|4.025|5.456|3.437|4.967|2.779|6.104|6.510|5.922|5.275|2.564|3.568|5.229|4.532|5.761|3.802\\nm|2.406|2.968|2.534|3.157|4.738|2.994|2.512|3.752|3.471|2.515|2.213|3.237|4.041|3.202|2.446\\nn|6.749|7.095|9.776|6.712|4.446|7.955|6.883|7.987|8.542|6.237|10.032|7.240|7.711|8.826|6.468\\no|7.507|5.796|2.594|8.683|9.735|8.779|9.832|2.976|4.482|6.667|6.063|4.636|2.166|5.614|6.695\\np|1.929|2.521|0.670|2.510|2.523|2.755|3.056|0.886|1.839|2.445|1.57|1.756|0.789|1.842|1.906\\nq|0.095|1.362|0.018|0.877|1.204|0|0.505|0|0.020|0|0.009|0.007|0|0.013|0.001\\nr|5.987|6.693|7.003|6.871|6.530|5.914|6.367|7.722|8.431|5.243|6.411|8.956|8.581|2.872|4.799\\ns|6.327|7.948|7.270|7.977|6.805|6.092|4.981|3.014|6.590|5.224|3.73|5.805|5.630|7.862|5.212\\nt|9.056|7.244|6.154|4.632|4.336|5.276|5.623|3.314|7.691|2.475|6.79|6.862|4.953|8.750|5.727\\nu|2.758|6.311|4.166|2.927|3.639|3.183|3.011|3.235|1.919|2.062|1.99|1.979|4.562|5.008|2.160\\nv|0.978|1.838|0.846|1.138|1.575|1.904|2.097|0.959|2.415|0.012|2.85|2.332|2.437|2.250|5.344\\nw|2.360|0.074|1.921|0.017|0.037|0|0.033|0|0.142|5.813|1.52|0.069|0|0.094|0.016\\nx|0.150|0.427|0.034|0.215|0.253|0|0.003|0|0.159|0.004|0.036|0.028|0.046|0.031|0.027\\ny|1.974|0.128|0.039|1.008|0.006|0|0.020|3.336|0.708|3.206|0.035|0.698|0.900|1.745|1.043\\nz|0.074|0.326|1.134|0.467|0.470|0.494|1.181|1.500|0.070|4.852|1.39|0.034|0|0.051|1.503\\nà|0|0.486|0|0|0.072|0|0.635|0|0|0|0|0|0|0|0\\nâ|0|0.051|0|0|0.562|0|0|0|0|0|0|0|0|0|0\\ná|0|0|0|0.502|0.118|0|0|0|0|0|0|0|1.799|0|0.867\\nå|0|0|0|0|0|0|0|0|1.338|0|0|1.190|0|0.003|0\\nä|0|0|0.578|0|0|0|0|0|1.797|0|0|0|0|3.577|0\\nã|0|0|0|0|0.733|0|0|0|0|0|0|0|0|0|0\\ną|0|0|0|0|0|0|0|0|0|0.699|0|0|0|0|0\\næ|0|0|0|0|0|0|0|0|0|0|0|0.872|0.867|0|0\\nœ|0|0.018|0|0|0|0|0|0|0|0|0|0|0|0|0\\nç|0|0.085|0|0|0.530|0|0|1.156|0|0|0|0|0|0|0\\nĉ|0|0|0|0|0|0.657|0|0|0|0|0|0|0|0|0\\nć|0|0|0|0|0|0|0|0|0|0.743|0|0|0|0|0\\nč|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0.462\\nď|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0.015\\nð|0|0|0|0|0|0|0|0|0|0|0|0|4.393|0|0\\nè|0|0.271|0|0|0|0|0.263|0|0|0|0|0|0|0|0\\né|0|1.504|0|0.433|0.337|0|0|0|0|0|0|0|0.647|0|0.633\\nê|0|0.218|0|0|0.450|0|0|0|0|0|0|0|0|0|0\\në|0|0.008|0|0|0|0|0|0|0|0|0|0|0|0|0\\nę|0|0|0|0|0|0|0|0|0|1.035|0|0|0|0|0\\ně|0|0|0|0|0|0|0|0|0|0|0|0|0|0|1.222\\nĝ|0|0|0|0|0|0.691|0|0|0|0|0|0|0|0|0\\nğ|0|0|0|0|0|0|0|1.125|0|0|0|0|0|0|0\\nĥ|0|0|0|0|0|0.022|0|0|0|0|0|0|0|0|0\\nî|0|0.045|0|0|0|0|0|0|0|0|0|0|0|0|0\\nì|0|0|0|0|0|0|0.030|0|0|0|0|0|0|0|0\\ní|0|0|0|0.725|0.132|0|0|0|0|0|0|0|1.570|0|1.643\\nï|0|0.005|0|0|0|0|0|0|0|0|0|0|0|0|0\\nı|0|0|0|0|0|0|0|5.114|0|0|0|0|0|0|0\\nĵ|0|0|0|0|0|0.055|0|0|0|0|0|0|0|0|0\\nł|0|0|0|0|0|0|0|0|0|2.109|0|0|0|0|0\\nñ|0|0|0|0.311|0|0|0|0|0|0|0|0|0|0|0\\nń|0|0|0|0|0|0|0|0|0|0.362|0|0|0|0|0\\nň|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0.007\\nò|0|0|0|0|0|0|0.002|0|0|0|0|0|0|0|0\\nö|0|0|0.443|0|0|0|0|0.777|1.305|0|0|0|0.777|0.444|0\\nô|0|0.023|0|0|0.635|0|0|0|0|0|0|0|0|0|0\\nó|0|0|0|0.827|0.296|0|0|0|0|1.141|0|0|0.994|0|0.024\\nõ|0|0|0|0|0.040|0|0|0|0|0|0|0|0|0|0\\nø|0|0|0|0|0|0|0|0|0|0|0|0.939|0|0|0\\nř|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0.380\\nŝ|0|0|0|0|0|0.385|0|0|0|0|0|0|0|0|0\\nş|0|0|0|0|0|0|0|1.780|0|0|0|0|0|0|0\\nś|0|0|0|0|0|0|0|0|0|0.814|0|0|0|0|0\\nš|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0.688\\nß|0|0|0.307|0|0|0|0|0|0|0|0|0|0|0|0\\nť|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0.006\\nþ|0|0|0|0|0|0|0|0|0|0|0|0|1.455|0|0\\nù|0|0.058|0|0|0|0|0.166|0|0|0|0|0|0|0|0\\nú|0|0|0|0.168|0.207|0|0|0|0|0|0|0|0.613|0|0.045\\nû|0|0.060|0|0|0|0|0|0|0|0|0|0|0|0|0\\nŭ|0|0|0|0|0|0.520|0|0|0|0|0|0|0|0|0\\nü|0|0|0.995|0.012|0.026|0|0|1.854|0|0|0|0|0|0|0\\nů|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0.204\\ný|0|0|0|0|0|0|0|0|0|0|0|0|0.228|0|0.995\\nź|0|0|0|0|0|0|0|0|0|0.078|0|0|0|0|0\\nż|0|0|0|0|0|0|0|0|0|0.706|0|0|0|0|0\\nž|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0.721\"\"\"\n",
    "df = pd.read_table(StringIO(wiki_table), sep='|', index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the language classifier and answer the following:\n",
    "\n",
    "1. **[0.5p]** Naive Bayes can be implemented\n",
    "    either by multiplying probabilities or by adding\n",
    "    log-probabilities. Which one is better and why?\n",
    "2. **[1.5p]** What is the language of the following phrases, according to the classifier (below in a code cell)? Assume equal prior language probabilities $P(C)$.\n",
    "3. **[0-1 bonus]** What happens when a Naive Bayes classifier\n",
    "      is applied to a document with out-of-vocabulary words? Propose\n",
    "      some solutions. Relate them to the concept of Bayesian\n",
    "      priors discussed during the lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    u\"No dejes para mañana lo que puedas hacer hoy.\",\n",
    "    u\"Przed wyruszeniem w drogę należy zebrać drużynę.\",\n",
    "    u\"Żeby zrozumieć rekurencję, należy najpierw zrozumieć rekurencję.\",\n",
    "    u\"Si vale la pena hacerlo vale la pena hacerlo bien.\",\n",
    "    u\"Experience is what you get when you didn't get what you wanted.\",\n",
    "    u\"Należy prowokować intelekt, nie intelektualistów.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Languages: English,French,German,Spanish,Portuguese,Esperanto,Italian,Turkish,Swedish,Polish,Dutch,Danish,Icelandic,Finnish,Czech\n",
      "Letters: a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, p, q, r, s, t, u, v, w, x, y, z, à, â, á, å, ä, ã, ą, æ, œ, ç, ĉ, ć, č, ď, ð, è, é, ê, ë, ę, ě, ĝ, ğ, ĥ, î, ì, í, ï, ı, ĵ, ł, ñ, ń, ň, ò, ö, ô, ó, õ, ø, ř, ŝ, ş, ś, š, ß, ť, þ, ù, ú, û, ŭ, ü, ů, ý, ź, ż, ž\n",
      "P(ę|Polish) = 1.035\n",
      "[ 0.06666667  0.06666667  0.06666667  0.06666667  0.06666667  0.06666667\n",
      "  0.06666667  0.06666667  0.06666667  0.06666667  0.06666667  0.06666667\n",
      "  0.06666667  0.06666667  0.06666667]\n",
      "Spanish : No dejes para mañana lo que puedas hacer hoy.\n",
      "Polish : Przed wyruszeniem w drogę należy zebrać drużynę.\n",
      "Polish : Żeby zrozumieć rekurencję, należy najpierw zrozumieć rekurencję.\n",
      "Italian : Si vale la pena hacerlo vale la pena hacerlo bien.\n",
      "English : Experience is what you get when you didn't get what you wanted.\n",
      "Polish : Należy prowokować intelekt, nie intelektualistów.\n"
     ]
    }
   ],
   "source": [
    "# We can easily access the data.\n",
    "langs = list(df)\n",
    "letters = list(df.index)\n",
    "print 'Languages:', ','.join(langs)\n",
    "print 'Letters:', ', '.join(letters)\n",
    "print u'P(ę|Polish) =', df.loc[u'ę'.encode('utf-8'), 'Polish']\n",
    "\n",
    "# Normalize values to a probability distribution\n",
    "df_norm = df.divide(df.values.sum(0) * len(langs))\n",
    "print df_norm.values.sum(0)\n",
    "\n",
    "def naive_bayes(sent, langs, df_norm):\n",
    "    \"\"\"Returns the most probable language of a sentence\"\"\"\n",
    "    probabilities = {}\n",
    "    for lang in langs:\n",
    "        probability = 1\n",
    "        for symbol in sent.lower():\n",
    "            if(symbol.encode('utf-8') in letters): \n",
    "                probability *= df_norm.loc[symbol.encode('utf-8'), lang]\n",
    "        probabilities[lang] = probability;\n",
    "    return max(probabilities, key=probabilities.get)\n",
    "    \n",
    "    \n",
    "\n",
    "for sent in sentences:\n",
    "    print naive_bayes(sent, langs, df_norm), ':', sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answers:\n",
    "**1:** Probabilities are often small numbers. To calculate joint probabilities, we need to multiply probabilities together. When we multiply one small number by another small number, we get a very small number. It is possible to get into difficulty with the precision of our floating point values, such as under-runs. To avoid this problem, it is more preferrable to work in the log probability space (take the logarithm of our probabilities).\n",
    "(<a href=https://machinelearningmastery.com/better-naive-bayes/>Source</a>)\n",
    "\n",
    "**2:** <br>\n",
    "Spanish : No dejes para mañana lo que puedas hacer hoy. <br>\n",
    "Polish : Przed wyruszeniem w drogę należy zebrać drużynę. <br>\n",
    "Polish : Żeby zrozumieć rekurencję, należy najpierw zrozumieć rekurencję. <br>\n",
    "Italian : Si vale la pena hacerlo vale la pena hacerlo bien. <br>\n",
    "English : Experience is what you get when you didn't get what you wanted. <br>\n",
    "Polish : Należy prowokować intelekt, nie intelektualistów. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3 [2p]\n",
    "Given observations $x_1,\\ldots,x_n$\n",
    "  coming from a certain distribution,\n",
    "  prove that MLE of a particular parameter of that distribution is equal to the sample mean $\\frac{1}{n}\\sum_{i=1}^n x_i$:\n",
    "1. Bernoulli distribution with success probability $p$ and MLE $\\hat{p}$,\n",
    "2. Gaussian distribution $\\mathcal{N}(\\mu,\\sigma)$ and MLE $\\hat{\\mu}$,\n",
    "3. Poisson distribution $\\mathit{Pois}(\\lambda)$ and MLE $\\hat{\\lambda}$.\n",
    "\n",
    "*NOTE: You can submit your solution on paper. To freeze the solution and spend late days, at minimum send a photo of your solution via e-mail.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://scontent-frt3-1.xx.fbcdn.net/v/t34.0-12/22782413_10214216455355303_1232229290_n.png?oh=11d5aeb73524a6727c48c7f875b9be7b&oe=59F12ACC\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url= \"https://scontent-frt3-1.xx.fbcdn.net/v/t34.0-12/22782413_10214216455355303_1232229290_n.png?oh=11d5aeb73524a6727c48c7f875b9be7b&oe=59F12ACC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4 [2p]\n",
    "\n",
    "1. Find simple expressions for the following functions' derivatives with respect to vector $\\mathbf{x}$.\n",
    "    1. $\\tanh(\\mathbf{x})$\n",
    "    2. $\\sigma(\\mathbf{x}) = \\frac{1}{1 + e^{-\\mathbf{x}}}$\n",
    "2. Find the following functions' gradients with respect to vector $[x, y, z]^T$:\n",
    "    1. $f_1([x, y, z]^T) = x + y$\n",
    "    2. $f_2([x, y, z]^T) = xy$\n",
    "    3. $f_3([x, y, z]^T) = x^2y^2$\n",
    "    4. $f_4([x, y, z]^T) = (x + y)^2$\n",
    "    5. $f_5([x, y, z]^T) = x^4 + x^2 y z + x y^2 z + z^4$\n",
    "    6. $f_6([x, y, z]^T) = e^{x + 2y}$\n",
    "    7. $f_7([x, y, z]^T) = \\frac{1}{x y^2}$\n",
    "    8. $f_8([x, y, z]^T) = ax + by + c$\n",
    "    9. $f_9([x, y, z]^T) = \\tanh(ax + by + c)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Derivatives of tanh: ', array([ 0.41997434,  0.55905517,  0.71157776,  0.85563879,  0.96104298,\n",
      "        1.        ,  0.96104298,  0.85563879,  0.71157776,  0.55905517,\n",
      "        0.41997434]))\n",
      "('Derivatives of sigma: ', array([ 0.19661193,  0.2139097 ,  0.22878424,  0.24026075,  0.24751657,\n",
      "        0.25      ,  0.24751657,  0.24026075,  0.22878424,  0.2139097 ,\n",
      "        0.19661193]))\n",
      "Gradients:\n",
      "[ 0.75  0.75  0.75]\n",
      "[ 0.85714286  0.85714286  0.85714286]\n",
      "[ 0.12244898  0.12244898  0.12244898]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.misc import derivative \n",
    "\n",
    "#1:\n",
    "x = np.linspace(-1, 1, 11)\n",
    "def sigma(n):\n",
    "    return 1/(1+np.exp(-n))\n",
    "print(\"Derivatives of tanh: \", derivative(np.tanh,x,dx=1e-6))\n",
    "print(\"Derivatives of sigma: \", derivative(sigma,x,dx=1e-6))\n",
    "\n",
    "#2:\n",
    "vector = np.array([1,7,13])\n",
    "transposed = vector.reshape(-1,1)\n",
    "print(\"Gradients:\")\n",
    "print(np.gradient(vector, vector[0]+vector[1]))\n",
    "print(np.gradient(vector, vector[0]*vector[1]))\n",
    "print(np.gradient(vector, vector[0]**2*vector[1]**2))\n",
    "#..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 5 [2p]\n",
    "\n",
    "Find the following functions' gradients or Jacobians with respect to vector $\\mathbf{x}$, where $\\mathbf{x}, \\mathbf{b} \\in \\mathbb{R}^{n}$, $\\mathbf{W} \\in \\mathbb{R}^{n \\times n}$:\n",
    "\n",
    "1. **[0.5p]** $\\mathbf{W} \\mathbf{x} + \\mathbf{b}$\n",
    "2. **[0.5p]** $\\mathbf{x}^T \\mathbf{W} \\mathbf{x}$,\n",
    "3. **[0.5p]** $\\sigma(\\mathbf{W} \\mathbf{x} + \\mathbf{b})$,\n",
    "    with $\\sigma$ applied element-wise. Hint: use the Hadamard product\n",
    "    (denoted $\\circ$) for element-wise matrix multiplication.\n",
    "4. **[0.5p]** $-\\log(S(\\mathbf{x})_j)$, where $S$ is the\n",
    "    softmax function\n",
    "    (https://en.wikipedia.org/wiki/Softmax_function) and we are\n",
    "    interested in the derivative over the $j$-th output of the\n",
    "    Softmax."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
